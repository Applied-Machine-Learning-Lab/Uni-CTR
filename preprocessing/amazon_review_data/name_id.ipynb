{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T13:13:12.463675Z",
     "start_time": "2023-12-18T13:13:12.461509Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import swifter\n",
    "\n",
    "\n",
    "def print_elapsed_time(start_time, segment_name):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken for {segment_name}: {elapsed_time / 60:.2f} minutes\")\n",
    "    return time.time()  # 返回当前时间，以便于下次计时\n",
    "\n",
    "\n",
    "def process_nan(data, nan_list, fill_text=\"unknown\"):\n",
    "    for col in nan_list:\n",
    "        # Fill NaN values with 'unknown'\n",
    "        data[col].fillna(fill_text, inplace=True)\n",
    "        # Replace empty strings with 'unknown'\n",
    "        data[col] = data[col].apply(lambda x: fill_text if x == \"\" else x)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_price(data):\n",
    "    x = data['price']\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return None\n",
    "    else:\n",
    "        found_price = re.match(r'^\\$\\d+(\\.\\d+)?$', x)\n",
    "        if found_price:\n",
    "            price = float(found_price.group()[1:])  # 提取数字部分并转换为浮点数\n",
    "            return price\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_user_history_feature_optimized(data, time_window, mode=\"id\"):\n",
    "    # Sort the data by 'user_id' and 'unixReviewTime'\n",
    "    data = data.sort_values(by=['user_id', 'unixReviewTime']).reset_index(drop=True)\n",
    "\n",
    "    # Create an empty 'user_hist' column\n",
    "    data['user_hist'] = ''\n",
    "\n",
    "    # Create a dictionary to hold the user's history\n",
    "    user_histories = {}\n",
    "\n",
    "    # Create a dictionary to map asin to title for fast lookup\n",
    "    if mode == \"title\":\n",
    "        asin_to_title = {row['asin']: str(row['title']) if pd.notna(row['title']) else \"unknown\"\n",
    "                         for _, row in data.iterrows()}\n",
    "\n",
    "    # Using tqdm to show the progress bar\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0], desc=\"Processing user history\"):\n",
    "        user_id = row['user_id']\n",
    "        if user_id not in user_histories:\n",
    "            user_histories[user_id] = []\n",
    "\n",
    "        # Get the recent history based on the time_window\n",
    "        recent_history = user_histories[user_id][-time_window:]\n",
    "\n",
    "        # Convert the recent history to the desired format\n",
    "        if mode == \"id\":\n",
    "            history_str = ' & '.join(['product_' + str(asin) for asin in recent_history])\n",
    "        elif mode == \"title\":\n",
    "            # Use the asin_to_title mapping for fast lookup\n",
    "            titles = [asin_to_title[asin] for asin in recent_history]\n",
    "            history_str = ' & '.join([\"product '\" + title + \"'\" for title in titles])\n",
    "        else:\n",
    "            history_str = ''\n",
    "\n",
    "        # Assign the history string to the 'user_hist' column\n",
    "        data.at[idx, 'user_hist'] = history_str\n",
    "\n",
    "        # If the rating is positive, add the product to the user's history AFTER updating the user_hist column\n",
    "        if row['rating'] == 1:\n",
    "            user_histories[user_id].append(row['asin'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def click_process(user_hist_str):\n",
    "    if pd.isna(user_hist_str):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(user_hist_str)\n",
    "\n",
    "\n",
    "def process_text(row):\n",
    "    scenario_info = \"scenario_\" + str(row['scenario']) + \", \"\n",
    "\n",
    "    user_info = \"user_\" + str(row['user_id']) + ', '\n",
    "    user_info += click_process(row['user_hist']) + \", \"\n",
    "\n",
    "    item_info = 'product_' + str(row['asin']) + \", \"\n",
    "    item_info += \"title_\" + str(row['title']) + \", \"\n",
    "    item_info += \"brand_\" + str(row['brand']) + \", \"\n",
    "    item_info += \"price_\" + str(row['new_price']) + \". \"\n",
    "\n",
    "    return scenario_info + user_info + item_info\n",
    "\n",
    "\n",
    "def process_data(review_data_path: list, meta_data_path: list, save_path=\"\", mode=\"id\"):\n",
    "    print(pd.__version__)\n",
    "    start_time = time.time()\n",
    "\n",
    "    review_data_list = []\n",
    "    meta_data_list = []\n",
    "    start_time = print_elapsed_time(start_time, \"Import data\")\n",
    "\n",
    "    for i in range(len(review_data_path)):\n",
    "        review_data = pd.read_json(review_data_path[i], lines=True)\n",
    "        review_data['scenario'] = i\n",
    "        review_data_list.append(review_data)\n",
    "        meta_data = pd.read_json(meta_data_path[i], lines=True)\n",
    "        meta_data_list.append(meta_data)\n",
    "    start_time = print_elapsed_time(start_time, \"Combine data\")\n",
    "\n",
    "    review_data = pd.concat(review_data_list)\n",
    "    meta_data = pd.concat(meta_data_list)\n",
    "\n",
    "    meta_data = process_nan(meta_data, ['brand', 'feature', 'description', 'similar_item', 'tech1', 'title'])\n",
    "    meta_data = meta_data.drop_duplicates(subset=['asin'])\n",
    "\n",
    "    join_data = pd.merge(review_data, meta_data, on='asin')\n",
    "    start_time = print_elapsed_time(start_time, \"Merge data\")\n",
    "\n",
    "    join_data['new_price'] = join_data.apply(process_price, axis=1)\n",
    "    join_data['new_price'] = join_data['new_price'].fillna(join_data['new_price'].mean())\n",
    "    start_time = print_elapsed_time(start_time, \"Process price\")\n",
    "\n",
    "    sparse_features = ['reviewerID', 'asin', 'brand', 'title']\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        lbe.fit(join_data[feat])\n",
    "        join_data[feat] = lbe.transform(join_data[feat])\n",
    "\n",
    "    join_data['rating'] = join_data['overall'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    join_data = join_data.rename(columns={'reviewerID': 'user_id'})\n",
    "    start_time = print_elapsed_time(start_time, \"Process sparse features\")\n",
    "\n",
    "    join_data = get_user_history_feature_optimized(join_data, 5, mode=mode)\n",
    "    start_time = print_elapsed_time(start_time, \"Process user history\")\n",
    "\n",
    "    # 处理原始数据\n",
    "    filter_data = join_data[[\"scenario\", \"user_id\", \"asin\", \"brand\", \"new_price\",\n",
    "                             \"rating\", \"unixReviewTime\", \"title\", \"user_hist\", \"overall\"]]\n",
    "    filter_data = filter_data.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # 生成文本数据\n",
    "    filter_data = process_nan(filter_data, ['user_hist'], fill_text=\"\")\n",
    "    filter_data['content'] = filter_data.swifter.apply(lambda row: process_text(row), axis=1)\n",
    "\n",
    "    # 过滤过长的文本数据\n",
    "    filter_data = filter_data[filter_data['content'].str.len() < 10000]\n",
    "\n",
    "    # 保存文本数据\n",
    "    print(\"Saving text data...\")\n",
    "    filter_data.to_csv(save_path, sep='\\t', index=None, encoding='utf-8')\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n",
      "Time taken for Import data: 0.00 minutes\n",
      "Time taken for Combine data: 0.77 minutes\n",
      "Time taken for Merge data: 0.09 minutes\n",
      "Time taken for Process price: 0.17 minutes\n",
      "Time taken for Process sparse features: 0.17 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing user history: 100%|██████████| 3073322/3073322 [01:13<00:00, 41816.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Process user history: 1.32 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 3073322/3073322 [00:26<00:00, 114424.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving text data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Amazon Fashion\n",
    "    Digital Music\n",
    "    Musical Instruments\n",
    "    Gift Cards\n",
    "    All Beauty\n",
    "    '''\n",
    "    process_data(\n",
    "        [\n",
    "            '../../datasets/amazon_review_data/raw_data/review_data/AMAZON_FASHION.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/review_data/Digital_Music.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/review_data/Musical_Instruments.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/review_data/Gift_Cards.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/review_data/All_Beauty.json',\n",
    "        ],\n",
    "        [\n",
    "            '../../datasets/amazon_review_data/raw_data/meta_data/meta_AMAZON_FASHION.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/meta_data/meta_Digital_Music.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/meta_data/meta_Musical_Instruments.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/meta_data/meta_Gift_Cards.json',\n",
    "            '../../datasets/amazon_review_data/raw_data/meta_data/meta_All_Beauty.json',\n",
    "        ],\n",
    "        save_path='../../datasets/amazon_review_data/ablation/name_id.csv',\n",
    "        mode=\"id\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T13:16:26.093547Z",
     "start_time": "2023-12-18T13:13:12.897265Z"
    }
   },
   "id": "b1fb881c2696e3cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8e76334c0475b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
